As knowledge bases move into the landscape of larger ontologies we must work on optimizing the performance of we tools.
As knowledge bases have terabytes of related data we must work on optimizing the performance of we tools.
we are easily tempted to buy bigger machines.
we are easily tempted to fill rooms with armies of little ones to address the scalability problem.
Yet, careful analysis and evaluation of the characteristics of we data— leads to dramatic improvements in performance.
the characteristics of we data— using metrics— often.
Firstly, are current scalable systems scalable enough?.
we found that for some as large as 500,000 classes ) it is hard to say because benchmarks obscure the load-time costs for materialization.
we found that for large ontologies ) it is hard to say because benchmarks obscure the load-time costs for materialization.
we found that for deep ontologies ) it is hard to say because benchmarks obscure the load-time costs for materialization.
Therefore, to expose the load-time costs for materialization, we have synthesized a set of more representative ontologies.
scalability how do we manage knowledge over time.
Secondly, in designing for scalability.
By optimizing for ontology evolution we have reduced the population time, including materialization, for the NCBO Resource Index, a knowledge base of 16.4 billion annotations.
By optimizing for data distribution we have reduced the population time, including materialization, for the NCBO Resource Index, a knowledge base of 16.4 billion annotations.
the NCBO Resource Index, a knowledge base of 16.4 billion annotations linking 2.4 million terms from 200 ontologies to 3.5 million data elements, from one week to less than one hour for one of the large datasets on the same machine.