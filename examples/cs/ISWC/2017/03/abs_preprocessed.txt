Structured scene descriptions of images are useful for the automatic processing and querying of large image databases.
We show how the combination of a visual model can improve on the task of mapping images to associated scene description.
We show how the combination of a statistical semantic model can improve on the task of mapping images to associated scene description.
In this paper we consider scene descriptions which are represented as a set of triples, where each triple consists of a pair of visual objects, which appear in the image, and the relationship between them.
We combine a standard visual model for based on convolutional neural networks, with a latent variable model for link prediction.
We compare We capability for visual relationship detection.
We apply multiple state-of-the-art link prediction methods.
One of the main advantages of link prediction methods is that We can also generalize to triples which have never been observed in the training data.
We experimental results on the recently published Stanford Visual Relationship dataset, a challenging real world dataset, show that the integration of using link prediction methods can significantly improve visual relationship detection.
We combined approach achieves superior performance compared to the state-of-the-art method from the Stanford computer vision group.