During the last few years, several knowledge graph embedding models have been devised in order to handle machine learning problems for knowledge graphs. Some of the models which were proven to be capable of inferring relational patterns, such as symmetry or transitivity, show lower performance in practice than those not allowing to infer those patterns. It is often unknown what factors contribute to such performance differences among KGE models in the inference of particular patterns. We develop the concept of a solution space as a factor that has a direct influence on the practical performance of knowledge graph embedding models as well as their capability to infer relational patterns. We showcase the effect of solution space on a newly proposed model dubbed SpacE^ss. We describe the theoretical considerations behind the solution space and evaluate our model against state-of-the-art models on a set of standard benchmarks namely WordNet and FreeBase.

