There is a variety of available approaches to learn graph node embeddings. One of their common underlying task is the generation of (biased) random walks that are then fed into representation learning techniques. Some techniques generate biased random walks by using structural information. Other approaches, also rely on some form of semantic information. While the former are purely structural, thus not fully considering knowledge available in semantically rich networks, the latter require complex inputs (e.g., metapaths) or only leverage node types that may not be available. The goal of this paper is to overcome these limitations by introducing NESP (Node Embeddings via Semantic Proximity), which features two main components. The first provides four different ways of biasing random walks by leveraging semantic relatedness between predicates. The second component focuses on refining (existing) embeddings by leveraging the notion of semantic proximity. This component iteratively refines an initial set of node embeddings imposing the embeddings of semantic neighboring nodes of a node to lie within a sphere of fixed radius. We discuss an extensive experimental evaluation and comparison with related work.
