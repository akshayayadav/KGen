Answering simple questions over knowledge graphs is a well-studied problem in question answering.
Previous approaches for this task built on recurrent and convolutional neural network based architectures that use pretrained word embeddings.
It was recently shown that finetuning pretrained transformer networks -LRB- e.g. both BERT -RRB- can outperform previous approaches on various natural language processing tasks.
In this work, we investigate how well both BERT provide BiLSTM-based models in limited-data scenarios.
In this work, we investigate how well both BERT performs on SimpleQuestions.
In this work, we investigate how well both BERT performs on SimpleQuestions.
In this work, we investigate how well both BERT provide an evaluation of both BERT.