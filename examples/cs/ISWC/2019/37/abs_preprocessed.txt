Link prediction has recently been a major focus of knowledge graphs.
a major focus of knowledge graphs aims at predicting missing links between entities to complement a major focus of knowledge graphs.
Most previous works only consider the triples, but the triples provide less information than the paths.
Although some works consider the semantic information -LRB- i.e. similar entities get similar representations -RRB- of the paths using the Word2Vec models, some works ignore the syntactic information -LRB- i.e. the order of entities and relations -RRB- of the paths.
In this paper, we propose RW-LMLM, a novel approach for link prediction.
RW-LMLM consists of a random walk algorithm for a major focus of knowledge graph -LRB- KG -RRB- and a language model-based link prediction model.
generated by KG are viewed as pseudo-sentences for model-based link prediction model training.
RW-LMLM can capture the semantic information in a major focus of knowledge graphs by considering relations.
RW-LMLM can capture the syntactic information in a major focus of knowledge graphs by considering relations.
RW-LMLM can capture the semantic information in a major focus of knowledge graphs by considering entities.
RW-LMLM can capture the syntactic information in a major focus of knowledge graphs by considering entities.
RW-LMLM can capture the syntactic information in a major focus of knowledge graphs by considering order information of the paths.
RW-LMLM can capture the semantic information in a major focus of knowledge graphs by considering order information of the paths.
Experimental results show that our method outperforms several state-of-the-art models on benchmark datasets.
Further analysis shows that our model is highly parameter efficient.